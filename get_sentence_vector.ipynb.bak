{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import codecs\n",
    "from tensorflow.contrib.layers.python.layers import initializers\n",
    "from tensorflow.contrib import estimator\n",
    "from bert import modeling\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_path = '/home/dutir_2t/wangyan/bert_sentence_vector/checkpoint/NCBI_BERT_pubmed_mimic_uncased_L-24_H-1024_A-16 (1)'\n",
    "root_path = '/home/dutir_2t/wangyan/bert_sentence_vector'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(root_path,'data/PPI.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(root_path,'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config_file = os.path.join(bert_path,'bert_config.json')\n",
    "init_checkpoint = os.path.join(bert_path,'bert_model.ckpt')\n",
    "vocab_file = os.path.join(bert_path,'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "Batch_Size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理语料并结构化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    '''\n",
    "    为一个句子构建一个单个的数据样本\n",
    "    '''\n",
    "    def __init__(self,guid,text):\n",
    "        '''\n",
    "        guid: example id,是唯一的样本id\n",
    "        text: 原始输入的句子\n",
    "        '''\n",
    "        self.guid = guid\n",
    "        self.text = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeature(object):\n",
    "    '''\n",
    "    输入数据的一个单独输入特征集合\n",
    "    '''\n",
    "    def __init__(self,input_ids,input_mask,segment_ids):\n",
    "        '''\n",
    "        input_ids: 输入句子中的每个单词的id\n",
    "        input_mask: 代表该单词是否要输入的BERT中\n",
    "        segment_ids: 句子分割标志，如果是第一个句子，则用0表示，如果是第二个句子，则用1表示\n",
    "        '''\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    '''\n",
    "    读入数据，并按照需求处理数据(根据自己的任务来定义)转换为BERT需要的输入格式\n",
    "    '''\n",
    "    def read_data(self,input_file):\n",
    "        '''\n",
    "        读取自己的数据，所以这块代码属于可修改的，将数据转换为二维list格式\n",
    "        '''\n",
    "        with codecs.open(input_file,'r',encoding='utf-8') as f:\n",
    "            lines = []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                lines.append(line)\n",
    "            \n",
    "            return lines \n",
    "    \n",
    "    def create_example(self, lines, set_type='train'):\n",
    "        '''\n",
    "        set_type: 字符串类型，用于设置guid，表明它是来自训练集，测试集还是验证集，默认是'train'\n",
    "        '''\n",
    "        examples = []\n",
    "        for i,line in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type,i)\n",
    "            text = tokenization.convert_to_unicode(line)\n",
    "            \n",
    "            examples.append(InputExample(guid=guid,text=text))\n",
    "        \n",
    "        return examples\n",
    "    \n",
    "    def get_examples(self,data_dir):\n",
    "        return self.create_example(self.read_data(data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 解析样本，将word转换成id，并结构化到InputFeature中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_single_example(index,example,max_seq_len,tokenizer):\n",
    "    '''\n",
    "    index: 样本的index，用于控制打印信息\n",
    "    example: 一个样本 \n",
    "    '''\n",
    "    textlist = example.text.split(' ')\n",
    "    tokens = []\n",
    "    for i,word in enumerate(textlist):\n",
    "        token = tokenizer.tokenize(word) #使用bert的wordpiece方式分词\n",
    "        tokens.extend(token)\n",
    "    \n",
    "    #截断序列\n",
    "    if len(tokens) >= max_seq_len-1:\n",
    "        tokens = tokens[0:(max_seq_len-2)]\n",
    "    ntokens = []\n",
    "    segment_ids = []\n",
    "    \n",
    "    ntokens.append('[CLS]') #添加句子的cls标志\n",
    "    segment_ids.append(0)\n",
    "    for i,token in enumerate(tokens):\n",
    "        ntokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    ntokens.append('[SEP]') #添加seg句子分割标志\n",
    "    segment_ids.append(0)\n",
    "    \n",
    "    input_ids = tokenizer.convert_tokens_to_ids(ntokens) #将序列中的token转换为id\n",
    "    input_mask = [1]*len(input_ids)\n",
    "    \n",
    "    #padding序列\n",
    "    while len(input_ids) < max_seq_len:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "    \n",
    "    assert len(input_ids) == max_seq_len\n",
    "    assert len(input_mask) == max_seq_len\n",
    "    assert len(segment_ids) == max_seq_len\n",
    "    \n",
    "    #打印部分数据信息\n",
    "    if index < 5:\n",
    "        tf.logging.info(\"*** Example ***\")\n",
    "        tf.logging.info(\"guid: %s\" % (example.guid))\n",
    "        tf.logging.info(\"tokens: %s\" % \" \".join([tokenization.printable_text(x) for x in tokens]))\n",
    "        tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "        tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "        tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "    \n",
    "    #结构化为一个类\n",
    "    \n",
    "    feature = InputFeature(input_ids=input_ids,input_mask=input_mask,segment_ids=segment_ids)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filed_based_convert_examples_to_features(examples,max_seq_len,tokenizer):\n",
    "    input_ids = []\n",
    "    input_mask = []\n",
    "    segment_ids = []\n",
    "    for index,example in enumerate(examples):\n",
    "        if index % 5000 == 0:\n",
    "            tf.logging.info(\"Writing example %d of %d\" % (index, len(examples)))\n",
    "        feature = convert_single_example(index=index,example=example,max_seq_len=max_seq_len,tokenizer=tokenizer)\n",
    "        \n",
    "        \n",
    "        input_ids.append(feature.input_ids) \n",
    "        input_mask.append(feature.input_mask) \n",
    "        segment_ids.append(feature.segment_ids) \n",
    "        \n",
    "        \n",
    "    return input_ids,input_mask,segment_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将数据加载到BERT模型中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载BERT配置文件\n",
    "bert_config = modeling.BertConfig.from_json_file(bert_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载数据，获得数据样本\n",
    "processor = DataProcessor()\n",
    "examples = processor.get_examples(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取BERT的分词器\n",
    "mytokenizer = tokenization.FullTokenizer(vocab_file=vocab_file,do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 19642\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: train-0\n",
      "INFO:tensorflow:tokens: a gen ##omic scanning using afl ##p to detect candidate lo ##ci under selection in the fin ##less por ##po ##ise - l ##rb - neo ##ph ##oca ##ena ph ##oca ##eno ##ides - rr ##b - . identifying lo ##ci under natural selection from gen ##omic surveys is of great interest in different research areas , stimulated by the increasing ease with large numbers of markers to gain a genome - wide perspective on differentiation . in this study , we searched for the genetic signatures of selection by screening 114 amplified fragment length poly ##morphism - l ##rb - afl ##p - rr ##b - markers in three fin ##less por ##po ##ise populations inhabit ##ing contrasting natural environments - l ##rb - freshwater and marine habitat - rr ##b - . comparing among three populations , four afl ##p lo ##ci exhibited f - l ##rb - st - rr ##b - values higher than 0 . 97 ##5 quan ##tile which might be in ##fer ##red to be under diver ##gent selection and two lo ##ci fell below the 0 . 02 ##5 quan ##tile which might be affected by balancing selection . although these lo ##ci were not supported with statistical significance in false discovery rate - l ##rb - f ##dr - rr ##b - analysis , the present study illustrated the potential of genome - wide surveys to identify specific genome regions or genes associated with freshwater adaptation of the fin ##less por ##po ##ise .\n",
      "INFO:tensorflow:input_ids: 101 1037 8991 22026 13722 2478 10028 2361 2000 11487 4018 8840 6895 2104 4989 1999 1996 10346 3238 18499 6873 5562 1011 1048 15185 1011 9253 8458 24755 8189 6887 24755 16515 8621 1011 25269 2497 1011 1012 12151 8840 6895 2104 3019 4989 2013 8991 22026 12265 2003 1997 2307 3037 1999 2367 2470 2752 1010 25194 2011 1996 4852 7496 2007 2312 3616 1997 16387 2000 5114 1037 13458 1011 2898 7339 2006 20582 1012 1999 2023 2817 1010 2057 9022 2005 1996 7403 16442 1997 4989 2011 11326 12457 26986 15778 3091 26572 19539 1011 1048 15185 1011 10028 2361 1011 25269 2497 1011 16387 1999 2093 10346 3238 18499 6873 5562 7080 21490 2075 22133 3019 10058 1011 1048 15185 1011 12573 1998 3884 6552 1011 25269 2497 1011 1012 13599 2426 2093 7080 1010 2176 10028 2361 8840 6895 8176 1042 1011 1048 15185 1011 2358 1011 25269 2497 1011 5300 3020 2084 1014 1012 5989 2629 24110 15286 2029 2453 2022 1999 7512 5596 2000 2022 2104 17856 11461 4989 1998 2048 8840 6895 3062 2917 1996 1014 1012 6185 2629 24110 15286 2029 2453 2022 5360 2011 20120 4989 1012 2348 2122 8840 6895 2020 2025 3569 2007 7778 7784 1999 6270 5456 3446 1011 1048 15185 1011 1042 13626 1011 25269 2497 1011 4106 1010 1996 2556 2817 7203 1996 4022 1997 13458 1011 2898 12265 2000 6709 3563 13458 4655 2030 9165 3378 2007 12573 6789 1997 1996 10346 3238 18499 6873 5562 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: train-1\n",
      "INFO:tensorflow:tokens: per ##ip ##at ##ric differentiation among adjacent marine lake and lagoon populations of a coastal fish , sp ##hae ##ram ##ia orb ##icular ##is - l ##rb - ap ##ogo ##nidae , per ##ci ##form ##es , tel ##eo ##ste ##i - rr ##b - . the effect of geographical isolation on spec ##iation , particularly within short geographical ranges , is poorly understood among marine organisms . focusing on marine lakes of the pal ##au islands , we investigated the effect of geographical isolation on sp ##hae ##ram ##ia orb ##icular ##is , a coastal fish inhabit ##ing marine lakes and lagoon ##s . we collected a total of 157 individuals from three mer ##omic ##tic marine lakes and three lagoon sites , and analyzed the genetic diversity and differentiation of the populations based on complete sequences of the mitochondrial control region - l ##rb - 82 ##4 bp - rr ##b - . the analyses show that the genetic diversity of marine lake populations is much lower than that of lagoon populations . moreover , a mis ##mat ##ch distribution analysis suggests that marine lake populations have experienced a decrease followed by a rapid expansion of their population size . these results reveal that marine lake populations have experienced severe founder and / or bottle ##neck events during the last thousand to tens of thousand years . pair ##wise phi - l ##rb - st - rr ##b - values ranged from 0 . 53 ##1 to 0 . 84 ##8 between marine lake and lagoon populations and from 0 . 42 ##9 to 0 . 870 among marine lake populations , indicating a high degree of genetic differentiation . we spec ##ulate that such per ##ip ##at ##ric differentiation between marine lake and lagoon populations was caused by a small number of individuals colon ##izing the lakes from the lagoon - l ##rb - founder event - rr ##b - followed by repetitive bottle ##neck events , such as those generated by the el ni ##o - southern os ##ci ##llation - l ##rb - en ##so - rr ##b - . so far , such high genetic diver ##gence ##s in extremely short geographical ranges - l ##rb - approximately 150 - 250 m - rr ##b - have scarcely been reported for marine organisms . we suggest that the marine lake is one of the good model of geographical isolation in marine organisms and each marine lake population is in the early stages of spec ##iation .\n",
      "INFO:tensorflow:input_ids: 101 2566 11514 4017 7277 20582 2426 5516 3884 2697 1998 15825 7080 1997 1037 5780 3869 1010 11867 25293 6444 2401 19607 21412 2483 1011 1048 15185 1011 9706 22844 29135 1010 2566 6895 14192 2229 1010 10093 8780 13473 2072 1011 25269 2497 1011 1012 1996 3466 1997 10056 12477 2006 28699 18963 1010 3391 2306 2460 10056 8483 1010 2003 9996 5319 2426 3884 11767 1012 7995 2006 3884 6597 1997 1996 14412 4887 3470 1010 2057 10847 1996 3466 1997 10056 12477 2006 11867 25293 6444 2401 19607 21412 2483 1010 1037 5780 3869 21490 2075 3884 6597 1998 15825 2015 1012 2057 5067 1037 2561 1997 17403 3633 2013 2093 21442 22026 4588 3884 6597 1998 2093 15825 4573 1010 1998 16578 1996 7403 8906 1998 20582 1997 1996 7080 2241 2006 3143 10071 1997 1996 23079 2491 2555 1011 1048 15185 1011 6445 2549 17531 1011 25269 2497 1011 1012 1996 16478 2265 2008 1996 7403 8906 1997 3884 2697 7080 2003 2172 2896 2084 2008 1997 15825 7080 1012 9308 1010 1037 28616 18900 2818 4353 4106 6083 2008 3884 2697 7080 2031 5281 1037 9885 2628 2011 1037 5915 4935 1997 2037 2313 2946 1012 2122 3463 7487 2008 3884 2697 7080 2031 5281 5729 3910 1998 1013 2030 5835 18278 2824 2076 1996 2197 4595 2000 15295 1997 4595 2086 1012 3940 14244 13569 1011 1048 15185 1011 2358 1011 25269 2497 1011 5300 15844 2013 1014 1012 5187 2487 2000 1014 1012 6391 2620 2090 3884 2697 1998 15825 7080 1998 2013 1014 1012 4413 2683 2000 1014 1012 28864 2426 3884 2697 7080 1010 8131 1037 2152 3014 1997 7403 20582 1012 2057 28699 9869 2008 2107 2566 11514 4017 7277 20582 2090 3884 2697 1998 15825 7080 2001 3303 2011 1037 2235 2193 1997 3633 16844 6026 1996 6597 2013 1996 15825 1011 1048 15185 1011 3910 2724 1011 25269 2497 1011 2628 2011 23563 5835 18278 2824 1010 2107 2004 2216 7013 2011 1996 3449 9152 2080 1011 2670 9808 6895 20382 1011 1048 15185 1011 4372 6499 1011 25269 2497 1011 1012 2061 2521 1010 2107 2152 7403 17856 17905 2015 1999 5186 2460 10056 8483 1011 1048 15185 1011 3155 5018 1011 5539 1049 1011 25269 2497 1011 2031 20071 2042 2988 2005 3884 11767 1012 2057 6592 2008 1996 3884 2697 2003 2028 1997 1996 2204 2944 1997 10056 12477 1999 3884 11767 1998 2169 3884 2697 2313 2003 1999 1996 2220 5711 1997 28699 18963 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: train-2\n",
      "INFO:tensorflow:tokens: photos ##yn ##thesis - dependent ca ##2 + influx and functional diversity between ph ##os ##ph ##oli ##pas ##es in the formation of cell polar ##ity in migrating cells of red algae . un ##ice ##ll ##ular sp ##ore cells , designated as mono ##sp ##ores - l ##rb - also called arch ##eo ##sp ##ores - rr ##b - , are well known as migrating plant cells , in which establishment of the anterior - posterior axis directs as ##ym ##metric ##al distribution of f - act ##in . since the mechanisms of cell polar ##ity formation are not yet fully el ##uc ##ida ##ted in mono ##sp ##ores , we investigated the roles of ph ##os ##ph ##oin ##osi ##ti ##de signaling systems and ca ##2 + mobilization in migration . although we have already found the critical involvement of ph ##os ##pha ##ti ##dy ##lino ##sit ##ol 3 - kinase in the establishment of cell polar ##ity , we recently demonstrated the important roles of extra ##cellular ca ##2 + influx , ph ##os ##ph ##oli ##pas ##e c - l ##rb - plc - rr ##b - and ph ##os ##ph ##oli ##pas ##e d - l ##rb - pl ##d - rr ##b - . the remarkable characteristics of these factors are that ca ##2 + influx depends on photos ##yn ##the ##tic activity and that plc and pl ##d play roles in the establishment and maintenance of cell polar ##ity , respectively . these findings could provide new insight into the regulation of migration in eu ##kar ##yo ##tic cells .\n",
      "INFO:tensorflow:input_ids: 101 7760 6038 25078 1011 7790 6187 2475 1009 18050 1998 8360 8906 2090 6887 2891 8458 10893 19707 2229 1999 1996 4195 1997 3526 11508 3012 1999 28636 4442 1997 2417 18670 1012 4895 6610 3363 7934 11867 5686 4442 1010 4351 2004 18847 13102 16610 1011 1048 15185 1011 2036 2170 7905 8780 13102 16610 1011 25269 2497 1011 1010 2024 2092 2124 2004 28636 3269 4442 1010 1999 2029 5069 1997 1996 15099 1011 15219 8123 23303 2004 24335 12589 2389 4353 1997 1042 1011 2552 2378 1012 2144 1996 10595 1997 3526 11508 3012 4195 2024 2025 2664 3929 3449 14194 8524 3064 1999 18847 13102 16610 1010 2057 10847 1996 4395 1997 6887 2891 8458 28765 20049 3775 3207 14828 3001 1998 6187 2475 1009 25580 1999 9230 1012 2348 2057 2031 2525 2179 1996 4187 6624 1997 6887 2891 21890 3775 5149 25226 28032 4747 1017 1011 21903 1999 1996 5069 1997 3526 11508 3012 1010 2057 3728 7645 1996 2590 4395 1997 4469 16882 6187 2475 1009 18050 1010 6887 2891 8458 10893 19707 2063 1039 1011 1048 15185 1011 15492 1011 25269 2497 1011 1998 6887 2891 8458 10893 19707 2063 1040 1011 1048 15185 1011 20228 2094 1011 25269 2497 1011 1012 1996 9487 6459 1997 2122 5876 2024 2008 6187 2475 1009 18050 9041 2006 7760 6038 10760 4588 4023 1998 2008 15492 1998 20228 2094 2377 4395 1999 1996 5069 1998 6032 1997 3526 11508 3012 1010 4414 1012 2122 9556 2071 3073 2047 12369 2046 1996 7816 1997 9230 1999 7327 6673 7677 4588 4442 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: train-3\n",
      "INFO:tensorflow:tokens: evidence for a role of he ##x ##oki ##nas ##es as conserved glucose sensors in both mono ##cot and di ##cot plant species . the role of the he ##x ##oki ##nas ##es - l ##rb - h ##x ##ks - rr ##b - as glucose - l ##rb - g ##lc - rr ##b - sensors has been mainly demonstrated for arab ##ido ##psis - l ##rb - arab ##ido ##psis tha ##lian ##a - rr ##b - h ##x ##k ##1 - l ##rb - at ##h ##x ##k ##1 - rr ##b - but has yet to be shown in other plant species . in our recent publication , we reported that two rice - l ##rb - or ##y ##za sat ##iva - rr ##b - h ##x ##ks , os ##h ##x ##k ##5 and os ##h ##x ##k ##6 , also function as g ##lc sensors . these two enzymes harbor both mitochondrial targeting peptide ##s - l ##rb - mt ##ps - rr ##b - and nuclear local ##ization signals - l ##rb - nl ##ss - rr ##b - , and we confirmed their dual - targeting ability to nuclei and mit ##och ##ond ##ria using g ##fp fusion experiments . consistently , it has been previously known that at ##h ##x ##k ##1 is predominantly associated with mit ##och ##ond ##ria but is also present in nuclei in vivo at app ##re ##cia ##ble levels . notably , the expression of os ##h ##x ##k ##5 , os ##h ##x ##k ##6 , or their catalytic ##ally inactive mutant all ##eles complement ##ed the arab ##ido ##psis glucose ins ##ens ##itive ##2 - l ##rb - gin ##2 - rr ##b - mutant . in addition , trans ##genic rice plants over ##ex ##pressing os ##h ##x ##k ##5 or os ##h ##x ##k ##6 exhibited hyper ##sen ##sit ##ive plant growth re ##tar ##dation and enhanced repression of the rub ##is ##co small subunit - l ##rb - rb ##cs - rr ##b - gene in response to glucose treatment . our results thus provided evidence that os ##h ##x ##k ##5 and os ##h ##x ##k ##6 can function as glucose sensors in rice . hence , the available current data suggest that the role of the h ##x ##ks as g ##lc sensors may be conserved in both mono ##cot and di ##cot plant species , and that the nuclear local ##ization of at ##h ##x ##k ##1 , os ##h ##x ##k ##5 and os ##h ##x ##k ##6 may be critical for g ##lc sensing and signaling .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 3350 2005 1037 2535 1997 2002 2595 23212 11649 2229 2004 19995 18423 13907 1999 2119 18847 24310 1998 4487 24310 3269 2427 1012 1996 2535 1997 1996 2002 2595 23212 11649 2229 1011 1048 15185 1011 1044 2595 5705 1011 25269 2497 1011 2004 18423 1011 1048 15185 1011 1043 15472 1011 25269 2497 1011 13907 2038 2042 3701 7645 2005 5424 13820 18409 1011 1048 15185 1011 5424 13820 18409 22794 15204 2050 1011 25269 2497 1011 1044 2595 2243 2487 1011 1048 15185 1011 2012 2232 2595 2243 2487 1011 25269 2497 1011 2021 2038 2664 2000 2022 3491 1999 2060 3269 2427 1012 1999 2256 3522 4772 1010 2057 2988 2008 2048 5785 1011 1048 15185 1011 2030 2100 4143 2938 11444 1011 25269 2497 1011 1044 2595 5705 1010 9808 2232 2595 2243 2629 1998 9808 2232 2595 2243 2575 1010 2036 3853 2004 1043 15472 13907 1012 2122 2048 16285 6496 2119 23079 14126 25117 2015 1011 1048 15185 1011 11047 4523 1011 25269 2497 1011 1998 4517 2334 3989 7755 1011 1048 15185 1011 17953 4757 1011 25269 2497 1011 1010 1998 2057 4484 2037 7037 1011 14126 3754 2000 23767 1998 10210 11663 15422 4360 2478 1043 22540 10077 7885 1012 10862 1010 2009 2038 2042 3130 2124 2008 2012 2232 2595 2243 2487 2003 9197 3378 2007 10210 11663 15422 4360 2021 2003 2036 2556 1999 23767 1999 24269 2012 10439 2890 7405 3468 3798 1012 5546 1010 1996 3670 1997 9808 2232 2595 2243 2629 1010 9808 2232 2595 2243 2575 1010 2030 2037 26244 3973 16389 15527 2035 26741 13711 2098 1996 5424 13820 18409 18423 16021 6132 13043 2475 1011 1048 15185 1011 18353 2475 1011 25269 2497 1011 15527 1012 1999 2804 1010 9099 16505 5785 4264 2058 10288 24128 9808 2232 2595 2243 2629 2030 9808 2232 2595 2243 2575 8176 23760 5054 28032 3512 3269 3930 2128 7559 20207 1998 9412 22422 1997 1996 14548 2483 3597 2235 24312 1011 1048 15185 1011 21144 6169 1011 25269 2497 1011 4962 1999 3433 2000 18423 3949 1012 2256 3463 2947 3024 3350 2008 9808 2232 2595 2243 2629 1998 9808 2232 2595 2243 2575 2064 3853 2004 18423 13907 1999 5785 1012 6516 1010 1996 2800 2783 2951 6592 2008 1996 2535 1997 1996 1044 2595 5705 2004 1043 15472 13907 2089 2022 19995 1999 2119 18847 24310 1998 4487 24310 3269 2427 1010 1998 2008 1996 4517 2334 3989 1997 2012 2232 2595 2243 2487 1010 9808 2232 2595 2243 2629 1998 9808 2232 2595 2243 2575 2089 2022 4187 2005 1043 15472 13851 1998 14828 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: train-4\n",
      "INFO:tensorflow:tokens: influence of poly ##pl ##oid ##y on insect herb ##ivo ##res of native and invasive gen ##otype ##s of solid ##ago gig ##ante ##a - l ##rb - as ##tera ##ceae - rr ##b - . herb ##ivo ##res are sensitive to the genetic structure of plant populations , as genetics under ##lies plant ph ##eno ##type and host quality . poly ##pl ##oid ##y is a widespread feature of ang ##ios ##per ##m genome ##s , yet few studies have examined how poly ##pl ##oid ##y influences herb ##ivo ##res . introduction to new ranges , with con ##se ##quent changes in selective regimes , can lead to evolution of changes in plant defensive characteristics and also affect herb ##ivo ##res . here , we examine how insect herb ##ivo ##res respond to poly ##pl ##oid ##y in solid ##ago gig ##ante ##a , using plants derived from both the native range - l ##rb - usa - rr ##b - and introduced range - l ##rb - europe - rr ##b - . s . gig ##ante ##a has 3 cy ##to ##type ##s in the us , with 2 of these present in europe . we performed bio ##ass ##ays with general ##ist - l ##rb - sp ##od ##optera ex ##ig ##ua - rr ##b - and specialist - l ##rb - tri ##rh ##ab ##da vi ##rga ##ta - rr ##b - leaf - feeding insects . insects were reared on detached leaves - l ##rb - sp ##od ##optera - rr ##b - or pot ##ted host plants - l ##rb - tri ##rh ##ab ##da - rr ##b - and mortality and mass were measured . tri ##rh ##ab ##da larvae showed little variation in survival or pup ##al mass at ##tri ##bu ##table to either cy ##to ##type or plant origin . sp ##od ##optera larvae were more sensitive to both cy ##to ##type and plant origin : they grew best on european te ##tra ##pl ##oids and poorly on us dip ##loid ##s - l ##rb - high mortality - rr ##b - and us te ##tra ##pl ##oids - l ##rb - low la ##rval mass - rr ##b - . these results show that both cy ##to ##type and plant origin influence insect herb ##ivo ##res , but that general ##ist and specialist insects may respond differently .\n",
      "INFO:tensorflow:input_ids: 101 3747 1997 26572 24759 9314 2100 2006 14211 12810 20984 6072 1997 3128 1998 17503 8991 26305 2015 1997 5024 23692 15453 12956 2050 1011 1048 15185 1011 2004 14621 9071 1011 25269 2497 1011 1012 12810 20984 6072 2024 7591 2000 1996 7403 3252 1997 3269 7080 1010 2004 14471 2104 11983 3269 6887 16515 13874 1998 3677 3737 1012 26572 24759 9314 2100 2003 1037 6923 3444 1997 17076 10735 4842 2213 13458 2015 1010 2664 2261 2913 2031 8920 2129 26572 24759 9314 2100 8092 12810 20984 6072 1012 4955 2000 2047 8483 1010 2007 9530 3366 15417 3431 1999 13228 25228 1010 2064 2599 2000 6622 1997 3431 1999 3269 5600 6459 1998 2036 7461 12810 20984 6072 1012 2182 1010 2057 11628 2129 14211 12810 20984 6072 6869 2000 26572 24759 9314 2100 1999 5024 23692 15453 12956 2050 1010 2478 4264 5173 2013 2119 1996 3128 2846 1011 1048 15185 1011 3915 1011 25269 2497 1011 1998 3107 2846 1011 1048 15185 1011 2885 1011 25269 2497 1011 1012 1055 1012 15453 12956 2050 2038 1017 22330 3406 13874 2015 1999 1996 2149 1010 2007 1016 1997 2122 2556 1999 2885 1012 2057 2864 16012 12054 22916 2007 2236 2923 1011 1048 15185 1011 11867 7716 25324 4654 8004 6692 1011 25269 2497 1011 1998 8325 1011 1048 15185 1011 13012 25032 7875 2850 6819 28921 2696 1011 25269 2497 1011 7053 1011 8521 9728 1012 9728 2020 23295 2006 12230 3727 1011 1048 15185 1011 11867 7716 25324 1011 25269 2497 1011 2030 8962 3064 3677 4264 1011 1048 15185 1011 13012 25032 7875 2850 1011 25269 2497 1011 1998 13356 1998 3742 2020 7594 1012 13012 25032 7875 2850 9673 3662 2210 8386 1999 7691 2030 26781 2389 3742 2012 18886 8569 10880 2000 2593 22330 3406 13874 2030 3269 4761 1012 11867 7716 25324 9673 2020 2062 7591 2000 2119 22330 3406 13874 1998 3269 4761 1024 2027 3473 2190 2006 2647 8915 6494 24759 17086 1998 9996 2006 2149 16510 27710 2015 1011 1048 15185 1011 2152 13356 1011 25269 2497 1011 1998 2149 8915 6494 24759 17086 1011 1048 15185 1011 2659 2474 26585 3742 1011 25269 2497 1011 1012 2122 3463 2265 2008 2119 22330 3406 13874 1998 3269 4761 3747 14211 12810 20984 6072 1010 2021 2008 2236 2923 1998 8325 9728 2089 6869 11543 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:Writing example 5000 of 19642\n",
      "INFO:tensorflow:Writing example 10000 of 19642\n",
      "INFO:tensorflow:Writing example 15000 of 19642\n"
     ]
    }
   ],
   "source": [
    "#获得input_ids,input_mask和segment_ids\n",
    "input_ids,input_mask,segment_ids = filed_based_convert_examples_to_features(examples,max_seq_len=MAX_LEN,tokenizer=mytokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对输入数据分块\n",
    "input_data = tf.data.Dataset.from_tensor_slices({\n",
    "    \"input_ids\": tf.constant(input_ids),\n",
    "    \"input_mask\": tf.constant(input_mask),\n",
    "    \"segment_ids\": tf.constant(segment_ids)\n",
    "})\n",
    "\n",
    "input_data = input_data.batch(Batch_Size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取可迭代项\n",
    "iterator = input_data.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(device_count={'GPU': 0},allow_soft_placement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载Batach0\n",
      "加载Batach1\n",
      "加载Batach2\n",
      "加载Batach3\n",
      "加载Batach4\n",
      "加载Batach5\n",
      "加载Batach6\n",
      "加载Batach7\n",
      "加载Batach8\n",
      "加载Batach9\n",
      "加载Batach10\n",
      "加载Batach11\n",
      "加载Batach12\n",
      "加载Batach13\n",
      "加载Batach14\n",
      "加载Batach15\n",
      "加载Batach16\n",
      "加载Batach17\n",
      "加载Batach18\n",
      "加载Batach19\n",
      "加载Batach20\n",
      "加载Batach21\n",
      "加载Batach22\n",
      "加载Batach23\n",
      "加载Batach24\n",
      "加载Batach25\n",
      "加载Batach26\n",
      "加载Batach27\n",
      "加载Batach28\n",
      "加载Batach29\n",
      "加载Batach30\n",
      "加载Batach31\n",
      "加载Batach32\n",
      "加载Batach33\n",
      "加载Batach34\n",
      "加载Batach35\n",
      "加载Batach36\n",
      "加载Batach37\n",
      "加载Batach38\n",
      "加载Batach39\n",
      "加载Batach40\n",
      "加载Batach41\n",
      "加载Batach42\n",
      "加载Batach43\n",
      "加载Batach44\n",
      "加载Batach45\n",
      "加载Batach46\n",
      "加载Batach47\n",
      "加载Batach48\n",
      "加载Batach49\n",
      "加载Batach50\n",
      "加载Batach51\n",
      "加载Batach52\n",
      "加载Batach53\n",
      "加载Batach54\n",
      "加载Batach55\n",
      "加载Batach56\n",
      "加载Batach57\n",
      "加载Batach58\n",
      "加载Batach59\n",
      "加载Batach60\n",
      "加载Batach61\n",
      "加载Batach62\n",
      "加载Batach63\n",
      "加载Batach64\n",
      "加载Batach65\n",
      "加载Batach66\n",
      "加载Batach67\n",
      "加载Batach68\n",
      "加载Batach69\n",
      "加载Batach70\n",
      "加载Batach71\n",
      "加载Batach72\n",
      "加载Batach73\n",
      "加载Batach74\n",
      "加载Batach75\n",
      "加载Batach76\n",
      "加载Batach77\n",
      "加载Batach78\n",
      "加载Batach79\n",
      "加载Batach80\n",
      "加载Batach81\n",
      "加载Batach82\n",
      "加载Batach83\n",
      "加载Batach84\n",
      "加载Batach85\n",
      "加载Batach86\n",
      "加载Batach87\n",
      "加载Batach88\n",
      "加载Batach89\n",
      "加载Batach90\n",
      "加载Batach91\n",
      "加载Batach92\n",
      "加载Batach93\n",
      "加载Batach94\n",
      "加载Batach95\n",
      "加载Batach96\n",
      "加载Batach97\n",
      "加载Batach98\n",
      "加载Batach99\n",
      "加载Batach100\n",
      "加载Batach101\n",
      "加载Batach102\n",
      "加载Batach103\n",
      "加载Batach104\n",
      "加载Batach105\n",
      "加载Batach106\n",
      "加载Batach107\n",
      "加载Batach108\n",
      "加载Batach109\n",
      "加载Batach110\n",
      "加载Batach111\n",
      "加载Batach112\n",
      "加载Batach113\n",
      "加载Batach114\n",
      "加载Batach115\n",
      "加载Batach116\n",
      "加载Batach117\n",
      "加载Batach118\n",
      "加载Batach119\n",
      "加载Batach120\n",
      "加载Batach121\n",
      "加载Batach122\n",
      "加载Batach123\n",
      "加载Batach124\n",
      "加载Batach125\n",
      "加载Batach126\n",
      "加载Batach127\n",
      "加载Batach128\n",
      "加载Batach129\n",
      "加载Batach130\n",
      "加载Batach131\n",
      "加载Batach132\n",
      "加载Batach133\n",
      "加载Batach134\n",
      "加载Batach135\n",
      "加载Batach136\n",
      "加载Batach137\n",
      "加载Batach138\n",
      "加载Batach139\n",
      "加载Batach140\n",
      "加载Batach141\n",
      "加载Batach142\n",
      "加载Batach143\n",
      "加载Batach144\n",
      "加载Batach145\n",
      "加载Batach146\n",
      "加载Batach147\n",
      "加载Batach148\n",
      "加载Batach149\n",
      "加载Batach150\n",
      "加载Batach151\n",
      "加载Batach152\n",
      "加载Batach153\n",
      "加载Batach154\n",
      "加载Batach155\n",
      "加载Batach156\n",
      "加载Batach157\n",
      "加载Batach158\n",
      "加载Batach159\n",
      "加载Batach160\n",
      "加载Batach161\n",
      "加载Batach162\n",
      "加载Batach163\n",
      "加载Batach164\n",
      "加载Batach165\n",
      "加载Batach166\n",
      "加载Batach167\n",
      "加载Batach168\n",
      "加载Batach169\n",
      "加载Batach170\n",
      "加载Batach171\n",
      "加载Batach172\n",
      "加载Batach173\n",
      "加载Batach174\n",
      "加载Batach175\n",
      "加载Batach176\n",
      "加载Batach177\n",
      "加载Batach178\n",
      "加载Batach179\n",
      "加载Batach180\n",
      "加载Batach181\n",
      "加载Batach182\n",
      "加载Batach183\n",
      "加载Batach184\n",
      "加载Batach185\n",
      "加载Batach186\n",
      "加载Batach187\n",
      "加载Batach188\n",
      "加载Batach189\n",
      "加载Batach190\n",
      "加载Batach191\n",
      "加载Batach192\n",
      "加载Batach193\n",
      "加载Batach194\n",
      "加载Batach195\n",
      "加载Batach196\n",
      "加载Batach197\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "#迭代输出结果\n",
    "one_input_data = iterator.get_next()\n",
    "input_block = []\n",
    "with tf.Session(config=config) as sess:\n",
    "    try:\n",
    "        i = 0\n",
    "        while True:\n",
    "            #获取batch中的数据\n",
    "            print(\"加载Batach\"+str(i))\n",
    "            a = sess.run(one_input_data)\n",
    "            input_block.append(a)\n",
    "            i=i+1\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"end\")\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0次运行BERT\n",
      "获取第0表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第1次运行BERT\n",
      "获取第1表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第2次运行BERT\n",
      "获取第2表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第3次运行BERT\n",
      "获取第3表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第4次运行BERT\n",
      "获取第4表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第5次运行BERT\n",
      "获取第5表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第6次运行BERT\n",
      "获取第6表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第7次运行BERT\n",
      "获取第7表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第8次运行BERT\n",
      "获取第8表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第9次运行BERT\n",
      "获取第9表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第10次运行BERT\n",
      "获取第10表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第11次运行BERT\n",
      "获取第11表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第12次运行BERT\n",
      "获取第12表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第13次运行BERT\n",
      "获取第13表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第14次运行BERT\n",
      "获取第14表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第15次运行BERT\n",
      "获取第15表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第16次运行BERT\n",
      "获取第16表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第17次运行BERT\n",
      "获取第17表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第18次运行BERT\n",
      "获取第18表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第19次运行BERT\n",
      "获取第19表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第20次运行BERT\n",
      "获取第20表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第21次运行BERT\n",
      "获取第21表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第22次运行BERT\n",
      "获取第22表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第23次运行BERT\n",
      "获取第23表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第24次运行BERT\n",
      "获取第24表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第25次运行BERT\n",
      "获取第25表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第26次运行BERT\n",
      "获取第26表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第27次运行BERT\n",
      "获取第27表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第28次运行BERT\n",
      "获取第28表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第29次运行BERT\n",
      "获取第29表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第30次运行BERT\n",
      "获取第30表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第31次运行BERT\n",
      "获取第31表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第32次运行BERT\n",
      "获取第32表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第33次运行BERT\n",
      "获取第33表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第34次运行BERT\n",
      "获取第34表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第35次运行BERT\n",
      "获取第35表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第36次运行BERT\n",
      "获取第36表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第37次运行BERT\n",
      "获取第37表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第38次运行BERT\n",
      "获取第38表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第39次运行BERT\n",
      "获取第39表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第40次运行BERT\n",
      "获取第40表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第41次运行BERT\n",
      "获取第41表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第42次运行BERT\n",
      "获取第42表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第43次运行BERT\n",
      "获取第43表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第44次运行BERT\n",
      "获取第44表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第45次运行BERT\n",
      "获取第45表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第46次运行BERT\n",
      "获取第46表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第47次运行BERT\n",
      "获取第47表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第48次运行BERT\n",
      "获取第48表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第49次运行BERT\n",
      "获取第49表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第50次运行BERT\n",
      "获取第50表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第51次运行BERT\n",
      "获取第51表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第52次运行BERT\n",
      "获取第52表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第53次运行BERT\n",
      "获取第53表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第54次运行BERT\n",
      "获取第54表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第55次运行BERT\n",
      "获取第55表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第56次运行BERT\n",
      "获取第56表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第57次运行BERT\n",
      "获取第57表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第58次运行BERT\n",
      "获取第58表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第59次运行BERT\n",
      "获取第59表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第60次运行BERT\n",
      "获取第60表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第61次运行BERT\n",
      "获取第61表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第62次运行BERT\n",
      "获取第62表示的shape为：\n",
      "[100, 300]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================================================\n",
      "第63次运行BERT\n",
      "获取第63表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第64次运行BERT\n",
      "获取第64表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第65次运行BERT\n",
      "获取第65表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第66次运行BERT\n",
      "获取第66表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第67次运行BERT\n",
      "获取第67表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第68次运行BERT\n",
      "获取第68表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第69次运行BERT\n",
      "获取第69表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第70次运行BERT\n",
      "获取第70表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第71次运行BERT\n",
      "获取第71表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第72次运行BERT\n",
      "获取第72表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第73次运行BERT\n",
      "获取第73表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第74次运行BERT\n",
      "获取第74表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第75次运行BERT\n",
      "获取第75表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第76次运行BERT\n",
      "获取第76表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第77次运行BERT\n",
      "获取第77表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第78次运行BERT\n",
      "获取第78表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第79次运行BERT\n",
      "获取第79表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第80次运行BERT\n",
      "获取第80表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第81次运行BERT\n",
      "获取第81表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第82次运行BERT\n",
      "获取第82表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第83次运行BERT\n",
      "获取第83表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第84次运行BERT\n",
      "获取第84表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第85次运行BERT\n",
      "获取第85表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第86次运行BERT\n",
      "获取第86表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第87次运行BERT\n",
      "获取第87表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第88次运行BERT\n",
      "获取第88表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第89次运行BERT\n",
      "获取第89表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第90次运行BERT\n",
      "获取第90表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第91次运行BERT\n",
      "获取第91表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第92次运行BERT\n",
      "获取第92表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第93次运行BERT\n",
      "获取第93表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第94次运行BERT\n",
      "获取第94表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第95次运行BERT\n",
      "获取第95表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第96次运行BERT\n",
      "获取第96表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第97次运行BERT\n",
      "获取第97表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第98次运行BERT\n",
      "获取第98表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第99次运行BERT\n",
      "获取第99表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第100次运行BERT\n",
      "获取第100表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第101次运行BERT\n",
      "获取第101表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第102次运行BERT\n",
      "获取第102表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第103次运行BERT\n",
      "获取第103表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第104次运行BERT\n",
      "获取第104表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第105次运行BERT\n",
      "获取第105表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第106次运行BERT\n",
      "获取第106表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第107次运行BERT\n",
      "获取第107表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第108次运行BERT\n",
      "获取第108表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第109次运行BERT\n",
      "获取第109表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第110次运行BERT\n",
      "获取第110表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第111次运行BERT\n",
      "获取第111表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第112次运行BERT\n",
      "获取第112表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第113次运行BERT\n",
      "获取第113表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第114次运行BERT\n",
      "获取第114表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第115次运行BERT\n",
      "获取第115表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第116次运行BERT\n",
      "获取第116表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第117次运行BERT\n",
      "获取第117表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第118次运行BERT\n",
      "获取第118表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第119次运行BERT\n",
      "获取第119表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第120次运行BERT\n",
      "获取第120表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第121次运行BERT\n",
      "获取第121表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第122次运行BERT\n",
      "获取第122表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第123次运行BERT\n",
      "获取第123表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第124次运行BERT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取第124表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第125次运行BERT\n",
      "获取第125表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第126次运行BERT\n",
      "获取第126表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第127次运行BERT\n",
      "获取第127表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第128次运行BERT\n",
      "获取第128表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第129次运行BERT\n",
      "获取第129表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第130次运行BERT\n",
      "获取第130表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第131次运行BERT\n",
      "获取第131表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第132次运行BERT\n",
      "获取第132表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第133次运行BERT\n",
      "获取第133表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第134次运行BERT\n",
      "获取第134表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第135次运行BERT\n",
      "获取第135表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第136次运行BERT\n",
      "获取第136表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第137次运行BERT\n",
      "获取第137表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第138次运行BERT\n",
      "获取第138表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第139次运行BERT\n",
      "获取第139表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第140次运行BERT\n",
      "获取第140表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第141次运行BERT\n",
      "获取第141表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第142次运行BERT\n",
      "获取第142表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第143次运行BERT\n",
      "获取第143表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第144次运行BERT\n",
      "获取第144表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第145次运行BERT\n",
      "获取第145表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第146次运行BERT\n",
      "获取第146表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第147次运行BERT\n",
      "获取第147表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第148次运行BERT\n",
      "获取第148表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第149次运行BERT\n",
      "获取第149表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第150次运行BERT\n",
      "获取第150表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第151次运行BERT\n",
      "获取第151表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第152次运行BERT\n",
      "获取第152表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第153次运行BERT\n",
      "获取第153表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第154次运行BERT\n",
      "获取第154表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第155次运行BERT\n",
      "获取第155表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第156次运行BERT\n",
      "获取第156表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第157次运行BERT\n",
      "获取第157表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第158次运行BERT\n",
      "获取第158表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第159次运行BERT\n",
      "获取第159表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第160次运行BERT\n",
      "获取第160表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第161次运行BERT\n",
      "获取第161表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第162次运行BERT\n",
      "获取第162表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第163次运行BERT\n",
      "获取第163表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第164次运行BERT\n",
      "获取第164表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第165次运行BERT\n",
      "获取第165表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第166次运行BERT\n",
      "获取第166表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第167次运行BERT\n",
      "获取第167表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第168次运行BERT\n",
      "获取第168表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第169次运行BERT\n",
      "获取第169表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第170次运行BERT\n",
      "获取第170表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第171次运行BERT\n",
      "获取第171表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第172次运行BERT\n",
      "获取第172表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第173次运行BERT\n",
      "获取第173表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第174次运行BERT\n",
      "获取第174表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第175次运行BERT\n",
      "获取第175表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第176次运行BERT\n",
      "获取第176表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第177次运行BERT\n",
      "获取第177表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第178次运行BERT\n",
      "获取第178表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第179次运行BERT\n",
      "获取第179表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第180次运行BERT\n",
      "获取第180表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第181次运行BERT\n",
      "获取第181表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第182次运行BERT\n",
      "获取第182表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第183次运行BERT\n",
      "获取第183表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第184次运行BERT\n",
      "获取第184表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第185次运行BERT\n",
      "获取第185表示的shape为：\n",
      "[100, 300]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================================================\n",
      "第186次运行BERT\n",
      "获取第186表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第187次运行BERT\n",
      "获取第187表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第188次运行BERT\n",
      "获取第188表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第189次运行BERT\n",
      "获取第189表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第190次运行BERT\n",
      "获取第190表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第191次运行BERT\n",
      "获取第191表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第192次运行BERT\n",
      "获取第192表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第193次运行BERT\n",
      "获取第193表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第194次运行BERT\n",
      "获取第194表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第195次运行BERT\n",
      "获取第195表示的shape为：\n",
      "[100, 300]\n",
      "=============================================================================================\n",
      "第196次运行BERT\n",
      "获取第196表示的shape为：\n",
      "[42, 300]\n",
      "=============================================================================================\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for i,ins in enumerate(input_block):\n",
    "    print(\"第\"+str(i)+\"次运行BERT\")\n",
    "    model = modeling.BertModel(config=bert_config,\n",
    "                                   is_training=True,\n",
    "                                   input_ids=tf.constant(ins[\"input_ids\"]),\n",
    "                                   input_mask=tf.constant(ins[\"input_mask\"]),\n",
    "                                   token_type_ids=tf.constant(ins[\"segment_ids\"]),\n",
    "                                   use_one_hot_embeddings=False)\n",
    "    with tf.Session(config=config) as session:\n",
    "        with tf.variable_scope('pooling'):\n",
    "            embedding = model.get_pooled_output()\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        print(\"获取第\"+str(i)+\"表示的shape为：\")\n",
    "        print(embedding.shape.as_list())\n",
    "        result.append(session.run(embedding))\n",
    "        print('=============================================================================================')\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "alllist = []\n",
    "for i in result:\n",
    "    for j in i:\n",
    "        alllist.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(alllist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19642, 300)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(os.path.join(output_path,'PPI300.txt'),a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
